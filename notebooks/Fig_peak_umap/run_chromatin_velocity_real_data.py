#!/usr/bin/env python3
"""
Chromatin Velocity Analysis - Real Zebrahub-Multiome Data

Applies chromatin velocity computation to the actual zebrahub-multiome datasets:
- peaks_by_pb_annotated_master.h5ad (640K x 190 peaks)
- cicero_connections_peaks_integrated_peaks.csv (5.4M co-accessibility pairs)
"""

import pandas as pd
import numpy as np
import scanpy as sc
import anndata as ad
import matplotlib.pyplot as plt
import sys
import gc
from pathlib import Path
sys.path.append('./scripts')
from chromatin_velocity import ChromatinVelocity

# Set memory limits and verbosity
sc.settings.max_memory = 16  # 16GB memory limit
sc.settings.verbosity = 2

def load_data_with_memory_optimization():
    """Load real datasets with memory optimization strategies."""
    print("="*60)
    print("LOADING REAL ZEBRAHUB-MULTIOME DATASETS")
    print("="*60)
    
    # Data paths
    peaks_pb_path = "/hpc/projects/data.science/yangjoon.kim/zebrahub_multiome/data/annotated_data/objects_v2/peaks_by_pb_annotated_master.h5ad"
    coaccess_path = "/hpc/projects/data.science/yangjoon.kim/zebrahub_multiome/data/processed_data/13_peak_umap_analysis/cicero_integrated_ATAC_v2/02_integrated_ATAC_v2_cicero_connections_peaks_integrated_peaks.csv"
    
    print(f"\n1. Loading peaks-by-pseudobulk data...")
    print(f"   Path: {peaks_pb_path}")
    
    # Load with backed mode for memory efficiency
    try:
        adata = sc.read_h5ad(peaks_pb_path, backed='r')
        print(f"   ‚úì Loaded in backed mode: {adata.shape}")
        
        # Convert to in-memory for processing (only if manageable size)
        if adata.n_vars <= 1000:  # Only load if ‚â§1000 peaks
            adata = adata.to_memory()
            print(f"   ‚úì Converted to memory: {adata.shape}")
        else:
            print(f"   ! Large dataset ({adata.n_vars} peaks), keeping in backed mode")
            
    except Exception as e:
        print(f"   ‚ùå Error loading peaks data: {e}")
        return None, None
    
    print(f"\n2. Loading co-accessibility data (chunked)...")
    print(f"   Path: {coaccess_path}")
    
    # Load co-accessibility data in chunks to avoid memory issues
    chunk_size = 100000  # 100K rows at a time
    coaccess_chunks = []
    
    try:
        # First, check the total file size
        total_rows = sum(1 for _ in open(coaccess_path)) - 1  # -1 for header
        print(f"   Total co-accessibility pairs: {total_rows:,}")
        
        # Load in chunks
        chunk_count = 0
        for chunk in pd.read_csv(coaccess_path, chunksize=chunk_size):
            chunk_count += 1
            print(f"   Loading chunk {chunk_count}: {len(chunk):,} rows")
            
            # Filter chunk immediately to save memory
            if 'coaccess' in chunk.columns:
                # Keep only high-confidence connections
                chunk_filtered = chunk[abs(chunk['coaccess']) >= 0.05].copy()
                coaccess_chunks.append(chunk_filtered)
                print(f"     Filtered to {len(chunk_filtered):,} high-confidence pairs")
            else:
                coaccess_chunks.append(chunk)
            
            # Limit total chunks to prevent memory overflow
            if chunk_count >= 50:  # Max 5M rows
                print(f"   Stopping at chunk {chunk_count} to prevent memory issues")
                break
        
        # Combine chunks
        print(f"   Combining {len(coaccess_chunks)} chunks...")
        coaccess_df = pd.concat(coaccess_chunks, ignore_index=True)
        del coaccess_chunks  # Free memory
        gc.collect()
        
        print(f"   ‚úì Combined co-accessibility data: {len(coaccess_df):,} pairs")
        
    except Exception as e:
        print(f"   ‚ùå Error loading co-accessibility data: {e}")
        return adata, None
    
    return adata, coaccess_df

def filter_data_for_analysis(adata, coaccess_df, max_peaks=500, max_pseudobulks=200):
    """Filter datasets to manageable size for initial analysis."""
    print(f"\n3. Filtering data for analysis (max {max_peaks} peaks, {max_pseudobulks} pseudobulks)...")
    
    # Filter peaks to most variable/interesting ones
    if adata.n_vars > max_peaks:
        print(f"   Filtering from {adata.n_vars} to {max_peaks} peaks...")
        
        # Calculate peak variability
        if hasattr(adata.X, 'toarray'):
            X_dense = adata.X.toarray()
        else:
            X_dense = adata.X
            
        peak_var = np.var(X_dense, axis=0)
        top_peak_indices = np.argsort(peak_var)[-max_peaks:]
        
        adata_filtered = adata[:, top_peak_indices].copy()
        print(f"   ‚úì Filtered to {max_peaks} most variable peaks")
    else:
        adata_filtered = adata.copy()
        print(f"   ‚úì Keeping all {adata.n_vars} peaks")
    
    # Filter pseudobulks if too many
    if adata_filtered.n_obs > max_pseudobulks:
        print(f"   Filtering from {adata_filtered.n_obs} to {max_pseudobulks} pseudobulks...")
        
        # Random sample for now, could be improved with biological criteria
        import random
        random.seed(42)
        selected_obs = random.sample(range(adata_filtered.n_obs), max_pseudobulks)
        adata_filtered = adata_filtered[selected_obs, :].copy()
        print(f"   ‚úì Filtered to {max_pseudobulks} pseudobulks")
    
    # Filter co-accessibility data to only include selected peaks
    selected_peaks = set(adata_filtered.var_names.tolist())
    print(f"   Filtering co-accessibility to {len(selected_peaks)} selected peaks...")
    
    coaccess_filtered = coaccess_df[
        coaccess_df['Peak1'].isin(selected_peaks) & 
        coaccess_df['Peak2'].isin(selected_peaks)
    ].copy()
    
    print(f"   ‚úì Filtered co-accessibility: {len(coaccess_filtered):,} pairs")
    
    return adata_filtered, coaccess_filtered

def run_chromatin_velocity_analysis(adata, coaccess_df):
    """Run the chromatin velocity computation on real data."""
    print(f"\n4. Running chromatin velocity analysis...")
    
    # Prepare data format
    print("   Preparing data format...")
    accessibility_matrix = adata.X.T  # (peaks x pseudobulks)
    peak_names = adata.var_names.tolist()
    pseudobulk_names = adata.obs_names.tolist()
    
    print(f"   ‚úì Accessibility matrix: {accessibility_matrix.shape}")
    print(f"   ‚úì Peak names: {len(peak_names)}")
    print(f"   ‚úì Pseudobulk names: {len(pseudobulk_names)}")
    
    # Convert co-accessibility DataFrame to matrix
    print("   Converting co-accessibility to matrix format...")
    peak_to_idx = {peak: i for i, peak in enumerate(peak_names)}
    n_peaks = len(peak_names)
    
    # Use sparse matrix for memory efficiency
    from scipy.sparse import lil_matrix
    coaccess_matrix = lil_matrix((n_peaks, n_peaks))
    
    connections_added = 0
    for _, row in coaccess_df.iterrows():
        peak1, peak2 = row['Peak1'], row['Peak2']
        score = row['coaccess'] if 'coaccess' in row else row['score']
        
        if peak1 in peak_to_idx and peak2 in peak_to_idx:
            i, j = peak_to_idx[peak1], peak_to_idx[peak2]
            coaccess_matrix[i, j] = score
            coaccess_matrix[j, i] = score  # Make symmetric
            connections_added += 1
    
    # Convert to dense for ChromatinVelocity (it expects dense arrays)
    coaccess_matrix = coaccess_matrix.toarray()
    
    print(f"   ‚úì Co-accessibility matrix: {coaccess_matrix.shape}")
    print(f"   ‚úì Connections added: {connections_added:,}")
    print(f"   ‚úì Non-zero entries: {np.count_nonzero(coaccess_matrix):,}")
    
    # Initialize ChromatinVelocity
    print("   Initializing ChromatinVelocity...")
    try:
        cv = ChromatinVelocity(
            peaks_accessibility=accessibility_matrix,
            peak_names=peak_names,
            pseudobulk_names=pseudobulk_names,
            coaccessibility_matrix=coaccess_matrix
        )
        print("   ‚úì ChromatinVelocity initialized successfully")
        
        # Compute velocity components
        print("   Computing velocity components...")
        cv.compute_velocity_components(
            normalize_accessibility=True,
            min_coaccess_score=0.1,
            max_connections=50  # Reduced for large dataset
        )
        
        # Compute velocity (unspliced - spliced)
        cv.velocity = cv.unspliced_counts - cv.spliced_counts
        
        print("   ‚úì Velocity computation completed successfully")
        
        return cv
        
    except Exception as e:
        print(f"   ‚ùå Error in velocity computation: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_and_save_results(cv, adata, output_dir="./results_real_data"):
    """Analyze results and save output."""
    print(f"\n5. Analyzing and saving results...")
    
    # Create output directory
    Path(output_dir).mkdir(exist_ok=True)
    
    # Basic statistics
    print("   Computing velocity statistics...")
    velocity_stats = {
        'n_peaks': cv.velocity.shape[0],
        'n_pseudobulks': cv.velocity.shape[1],
        'spliced_range': [float(cv.spliced_counts.min()), float(cv.spliced_counts.max())],
        'unspliced_range': [float(cv.unspliced_counts.min()), float(cv.unspliced_counts.max())],
        'velocity_range': [float(cv.velocity.min()), float(cv.velocity.max())],
        'mean_velocity_magnitude': float(np.sqrt(np.sum(cv.velocity**2, axis=0)).mean())
    }
    
    print(f"   Spliced range: {velocity_stats['spliced_range']}")
    print(f"   Unspliced range: {velocity_stats['unspliced_range']}")
    print(f"   Velocity range: {velocity_stats['velocity_range']}")
    print(f"   Mean velocity magnitude: {velocity_stats['mean_velocity_magnitude']:.3f}")
    
    # Save velocity statistics
    import json
    with open(f"{output_dir}/velocity_stats_real_data.json", 'w') as f:
        json.dump(velocity_stats, f, indent=2)
    
    # Create and save AnnData object
    print("   Creating AnnData object with velocity layers...")
    try:
        adata_velocity = cv.create_anndata_object()
        
        # Add original metadata
        if hasattr(adata, 'obs') and len(adata.obs.columns) > 0:
            # Map metadata from original adata to velocity object
            metadata_cols = ['celltype', 'timepoint']
            for col in metadata_cols:
                if col in adata.obs.columns:
                    adata_velocity.var[col] = adata.obs[col].values
        
        # Save results
        adata_velocity.write_h5ad(f"{output_dir}/chromatin_velocity_real_data.h5ad")
        print(f"   ‚úì Velocity results saved to {output_dir}/chromatin_velocity_real_data.h5ad")
        
        return adata_velocity, velocity_stats
        
    except Exception as e:
        print(f"   ‚ùå Error saving results: {e}")
        return None, velocity_stats

def main():
    """Main analysis pipeline."""
    print("CHROMATIN VELOCITY ANALYSIS - REAL ZEBRAHUB-MULTIOME DATA")
    print("="*70)
    
    # Load data
    adata, coaccess_df = load_data_with_memory_optimization()
    if adata is None or coaccess_df is None:
        print("‚ùå Failed to load data. Exiting.")
        return None
    
    # Filter to manageable size
    adata_filtered, coaccess_filtered = filter_data_for_analysis(
        adata, coaccess_df, max_peaks=300, max_pseudobulks=100
    )
    
    # Run analysis
    cv = run_chromatin_velocity_analysis(adata_filtered, coaccess_filtered)
    if cv is None:
        print("‚ùå Velocity computation failed. Exiting.")
        return None
    
    # Analyze and save results
    adata_velocity, stats = analyze_and_save_results(cv, adata_filtered)
    
    print("\n" + "="*70)
    print("CHROMATIN VELOCITY ANALYSIS COMPLETED SUCCESSFULLY!")
    print("="*70)
    print("‚úÖ Real data loading: PASSED")
    print("‚úÖ Data filtering: PASSED")
    print("‚úÖ Velocity computation: PASSED")
    print("‚úÖ Results export: PASSED")
    
    print(f"\nüìä FINAL RESULTS:")
    print(f"   Analyzed {stats['n_peaks']} peaks across {stats['n_pseudobulks']} pseudobulks")
    print(f"   Velocity magnitude range: {stats['velocity_range']}")
    print(f"   Mean velocity magnitude: {stats['mean_velocity_magnitude']:.3f}")
    
    return adata_velocity

if __name__ == "__main__":
    result = main()