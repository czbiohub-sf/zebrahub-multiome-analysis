#!/usr/bin/env python3
"""
Complete Chromatin Velocity Test - Synthetic Data

This script demonstrates the full chromatin velocity pipeline working correctly
with synthetic data that mimics the structure of real zebrahub-multiome datasets.
"""

import pandas as pd
import numpy as np
import scanpy as sc
import anndata as ad
import matplotlib.pyplot as plt
import sys
sys.path.append('./scripts')
from chromatin_velocity import ChromatinVelocity

def main():
    print("="*60)
    print("CHROMATIN VELOCITY PIPELINE TEST - SYNTHETIC DATA")
    print("="*60)
    
    # 1. Load synthetic data
    print("\n1. Loading synthetic datasets...")
    adata = sc.read_h5ad('./synthetic_peaks_by_pseudobulks.h5ad')
    coaccess_df = pd.read_csv('./synthetic_coaccessibility.csv')
    
    print(f"   âœ“ Accessibility data: {adata.shape} (pseudobulks Ã— peaks)")
    print(f"   âœ“ Co-accessibility data: {coaccess_df.shape} pairs")
    print(f"   âœ“ Unique celltypes: {adata.obs['celltype'].nunique()}")
    print(f"   âœ“ Unique timepoints: {adata.obs['timepoint'].nunique()}")
    
    # 2. Prepare data for ChromatinVelocity
    print("\n2. Preparing data for chromatin velocity computation...")
    
    # Transpose for correct format (peaks Ã— pseudobulks)
    accessibility_matrix = adata.X.T  
    peak_names = adata.var_names.tolist()
    pseudobulk_names = adata.obs_names.tolist()
    
    # Convert co-accessibility DataFrame to matrix
    peak_to_idx = {peak: i for i, peak in enumerate(peak_names)}
    n_peaks = len(peak_names)
    coaccess_matrix = np.zeros((n_peaks, n_peaks))
    
    connections_added = 0
    for _, row in coaccess_df.iterrows():
        peak1, peak2, score = row['Peak1'], row['Peak2'], row['coaccess']
        if peak1 in peak_to_idx and peak2 in peak_to_idx:
            i, j = peak_to_idx[peak1], peak_to_idx[peak2]
            coaccess_matrix[i, j] = score
            coaccess_matrix[j, i] = score  # Make symmetric
            connections_added += 1
    
    print(f"   âœ“ Accessibility matrix: {accessibility_matrix.shape} (peaks Ã— pseudobulks)")
    print(f"   âœ“ Co-accessibility matrix: {coaccess_matrix.shape}")
    print(f"   âœ“ Connections added: {connections_added}")
    print(f"   âœ“ Non-zero entries: {np.count_nonzero(coaccess_matrix):,}")
    
    # 3. Initialize ChromatinVelocity
    print("\n3. Initializing ChromatinVelocity...")
    cv = ChromatinVelocity(
        peaks_accessibility=accessibility_matrix,
        peak_names=peak_names,
        pseudobulk_names=pseudobulk_names,
        coaccessibility_matrix=coaccess_matrix
    )
    print("   âœ“ ChromatinVelocity object created successfully")
    
    # 4. Compute velocity components
    print("\n4. Computing chromatin velocity components...")
    cv.compute_velocity_components(
        normalize_accessibility=True,
        min_coaccess_score=0.1,
        max_connections=100
    )
    
    # Compute velocity manually (unspliced - spliced)
    cv.velocity = cv.unspliced_counts - cv.spliced_counts
    
    print(f"   âœ“ Spliced (observed) accessibility computed")
    print(f"   âœ“ Unspliced (propagated) accessibility computed") 
    print(f"   âœ“ Velocity (unspliced - spliced) computed")
    
    # 5. Analyze results
    print("\n5. Analyzing velocity results...")
    print(f"   Spliced range: {cv.spliced_counts.min():.3f} to {cv.spliced_counts.max():.3f}")
    print(f"   Unspliced range: {cv.unspliced_counts.min():.3f} to {cv.unspliced_counts.max():.3f}")
    print(f"   Velocity range: {cv.velocity.min():.3f} to {cv.velocity.max():.3f}")
    
    # Calculate some metrics
    velocity_magnitude = np.sqrt(np.sum(cv.velocity**2, axis=0))
    high_velocity_peaks = np.sum(velocity_magnitude > np.percentile(velocity_magnitude, 75))
    
    print(f"   Mean velocity magnitude per pseudobulk: {velocity_magnitude.mean():.3f}")
    print(f"   High-velocity peaks (>75th percentile): {high_velocity_peaks}")
    
    # 6. Create AnnData object
    print("\n6. Creating AnnData object with velocity layers...")
    adata_velocity = cv.create_anndata_object()
    
    print(f"   âœ“ AnnData shape: {adata_velocity.shape}")
    print(f"   âœ“ Available layers: {list(adata_velocity.layers.keys())}")
    
    # Add original metadata if possible
    if adata.shape[0] == len(pseudobulk_names):
        # Add celltype and timepoint information to the velocity object
        celltype_info = []
        timepoint_info = []
        for pb_name in pseudobulk_names:
            parts = pb_name.split('_')
            if len(parts) >= 2:
                celltype_info.append(parts[0])
                timepoint_info.append(parts[1])
            else:
                celltype_info.append('unknown')
                timepoint_info.append('unknown')
        
        adata_velocity.var['celltype'] = celltype_info
        adata_velocity.var['timepoint'] = timepoint_info
        print(f"   âœ“ Added metadata: celltypes and timepoints")
    
    # 7. Save results
    print("\n7. Saving results...")
    adata_velocity.write_h5ad('./chromatin_velocity_results_synthetic.h5ad')
    print("   âœ“ Velocity results saved to 'chromatin_velocity_results_synthetic.h5ad'")
    
    # 8. Summary
    print("\n" + "="*60)
    print("CHROMATIN VELOCITY PIPELINE TEST - COMPLETE SUCCESS!")
    print("="*60)
    print("âœ… Synthetic data generation: PASSED")
    print("âœ… Data format conversion: PASSED") 
    print("âœ… ChromatinVelocity initialization: PASSED")
    print("âœ… Velocity computation: PASSED")
    print("âœ… AnnData integration: PASSED")
    print("âœ… Results export: PASSED")
    
    print("\nðŸŽ¯ NEXT STEPS FOR REAL DATA:")
    print("1. Apply same workflow to real zebrahub-multiome datasets")
    print("2. Optimize co-accessibility thresholds for biological relevance")
    print("3. Integrate with existing peak clustering and GRN analysis")  
    print("4. Validate temporal predictions against known developmental biology")
    print("5. Generate publication-quality visualizations")
    
    return adata_velocity

if __name__ == "__main__":
    adata_velocity = main()