# Chromatin Velocity Analysis - Hybrid Approach

This directory contains a new implementation of chromatin velocity computation that addresses the limitations of the previous method.

## ‚ö° TL;DR - Quick Start

**Just want to run it?**
```bash
cd scripts/chrom_velo/
python 01_chrom_velo_notebook.py  # Or convert to .ipynb with jupytext
```

**What each file does:**
- üìì `01_chrom_velo_notebook.py` - **START HERE** - Main analysis notebook (runs everything)
- ‚öôÔ∏è `chrom_velo_core.py` - Core computation (imported by notebook)
- üìä `chrom_velo_viz.py` - Visualization functions (imported by notebook)
- üìñ `README.md` - This file (detailed documentation)
- üìÅ `archive/` - Old implementation (ignore for now)

**Key parameters to edit** (in notebook configuration section):
- `USE_GPU = True` - Enable GPU acceleration (requires CuPy/cuML, 10-50x faster)
- `SUBSET_PEAKS = 10000` - Use 10K peaks for quick testing (change to `None` for full dataset)
- `ALPHA = 0.7` - How much to weight temporal (0.7) vs co-accessibility (0.3)
- `VELOCITY_SCALE = 0.1` - Arrow size in plots (0.1 = small arrows for better directionality)
- `RUN_COMPARISON = True` - Compare temporal-only vs regularized velocity

**Output:** 6 figures + results.h5ad in `figures/chrom_velocity/`
**Timing:** ~2-5 min (10K peaks, GPU) | ~20-60 min (640K peaks, GPU) | ~10x slower on CPU

---

## üéØ Key Innovation

**Problem with old approach**: Averaged accessibility across DIFFERENT peak sets at each timepoint, causing all peaks at time t to receive identical velocity vectors. This captured global trends (promoters) but missed local, context-dependent dynamics (enhancers/intergenic).

**New hybrid approach**:
1. **Temporal tracking**: Track the SAME peaks across timepoints
2. **Co-accessibility regularization**: Use regulatory context to smooth velocity
3. **Local projection**: Project to 2D using local linear transformations
4. **GPU acceleration**: Optional CuPy/cuML for 10-50x speedup (especially K-NN search)

## üìÅ Files

### Core Modules
- **`chrom_velo_core.py`** - Core computation module implementing the hybrid approach
  - `ChromatinVelocityComputer` class with three main methods:
    - `compute_temporal_velocity()`: Track peaks over time, fit splines, compute derivatives
    - `compute_coaccessibility_regularization()`: Smooth using co-accessible peaks
    - `project_to_2d()`: Local PCA-based projection to 2D UMAP space

- **`chrom_velo_viz.py`** - Visualization utilities
  - `ChromatinVelocityVisualizer` class with methods for:
    - Streamplots showing velocity flow
    - Arrow/quiver plots showing velocity vectors
    - Magnitude distribution analysis
    - Temporal vs regularized comparison plots

### Interactive Notebook
- **`01_chrom_velo_notebook.py`** - Main Jupytext notebook for analysis
  - Complete workflow from data loading to visualization
  - Configurable parameters for testing
  - Subset mode for quick iteration
  - Comprehensive visualization suite

### Archive
- **`archive/`** - Old implementation files (from September 2024)
  - Preserved for reference and comparison

## üî¨ Method Comparison

| Aspect | Old Method | New Method |
|--------|------------|------------|
| **Peak tracking** | Average across DIFFERENT peaks at each timepoint | Track SAME peaks over time |
| **Velocity assignment** | All peaks at time t get identical velocity | Each peak has unique velocity trajectory |
| **Dimensionality reduction** | Global SVD (190D ‚Üí 2D, loses 98.9% variance) | Local PCA-based projection (preserves local structure) |
| **Co-accessibility** | Not used | Used for regularization |
| **Temporal handling** | Simple difference between timepoint averages | Spline fitting with derivatives |
| **Works well for** | Promoters (global signal) | All peak types (local dynamics) |

## üöÄ Quick Start

### 1. Convert Jupytext to Notebook (Optional)
```bash
cd scripts/chrom_velo/
jupytext --to notebook 01_chrom_velo_notebook.py
```

### 2. Run Interactive Notebook
```bash
jupyter notebook 01_chrom_velo_notebook.ipynb
```

Or run directly as script:
```bash
python 01_chrom_velo_notebook.py
```

### 3. Key Parameters

Edit these in the notebook configuration section:

```python
# GPU Acceleration
USE_GPU = True               # Enable GPU acceleration (requires CuPy/cuML)

# Velocity computation
ALPHA = 0.7                  # Weight for temporal (0.7) vs co-accessibility (0.3)
MIN_COACCESS_SCORE = 0.5     # Minimum co-accessibility threshold
MAX_CONNECTIONS = 100        # Maximum connections per peak for regularization
SMOOTHING_FACTOR = 0.5       # Spline smoothing (0-1, higher = smoother)

# Visualization
VELOCITY_SCALE = 0.1         # Scale factor for arrow/streamplot display

# Testing
SUBSET_PEAKS = 10000         # Use subset for quick testing (None = full dataset)
RUN_COMPARISON = True        # Compare temporal-only vs regularized
```

## üöÄ GPU Acceleration

**NEW: Optional GPU support for 10-50x speedup!**

### Requirements
```bash
# On GPU node, install CuPy and cuML
conda install -c rapidsai -c conda-forge cuml cupy
```

### What's Accelerated
- **K-NN search** (30 neighbors √ó 640K peaks): 20-50x faster with cuML
- **Distance computations**: 10-30x faster with CuPy
- **Matrix operations**: GPU-accelerated throughout pipeline

### Usage
```python
# Enable GPU in notebook
USE_GPU = True

# Or in Python script
from chrom_velo_core import ChromatinVelocityComputer
cv_computer = ChromatinVelocityComputer(
    adata=adata,
    coaccess_df=coaccess_df,
    use_gpu=True  # Automatically falls back to CPU if GPU unavailable
)
```

### Performance (640K peaks)
- **GPU (cuML)**: ~20-60 minutes
- **CPU (sklearn)**: ~3-10 hours
- **Speedup**: ~10-20x overall (K-NN search is the bottleneck)

### Automatic Fallback
If GPU is not available, the code automatically falls back to CPU with no code changes needed.

## üìä Output Files

All outputs saved to `figures/chrom_velocity/`:

### Figures
1. **`01_streamplot_peak_type.png`** - Streamplot colored by peak type
2. **`02_arrows_peak_type.png`** - Arrow plot showing velocity vectors
3. **`03_magnitude_distribution.png`** - Velocity magnitude distribution by peak type
4. **`04_temporal_vs_regularized.png`** - Comparison of temporal and regularized velocity
5. **`05_comprehensive_summary.png`** - 4-panel comprehensive figure
6. **`06_temporal_vs_regularized_comparison.png`** - Side-by-side comparison (if RUN_COMPARISON=True)

### Data
- **`chromatin_velocity_results.h5ad`** - AnnData with computed velocities
  - Layers:
    - `temporal_velocity`: Raw temporal derivatives (n_peaks √ó n_pseudobulks)
    - `regularized_velocity`: Co-accessibility smoothed velocity
  - Obsm:
    - `velocity_umap`: 2D projected velocity vectors (n_peaks √ó 2)
  - Uns:
    - `velocity_params`: Parameter settings used

## üîß Algorithm Details

### Step 1: Temporal Velocity Computation

For each peak, extract accessibility trajectory across timepoints:

```python
# Pseudobulk naming: {celltype_id}_{timepoint}
# Example: '0_0somites', '0_5somites', '0_10somites', ...

for peak_i in all_peaks:
    for celltype in celltypes:
        # Get accessibility values at all timepoints for this celltype
        timepoints = [0, 5, 10, 15, 20, 30]  # somites
        accessibility = [acc(peak_i, celltype, t) for t in timepoints]

        # Fit smooth spline (handles sparse timepoints)
        spline = fit_spline(timepoints, accessibility, smoothing=0.5)

        # Compute derivative at each timepoint
        velocity(peak_i, celltype, t) = spline.derivative()(t)
```

**Key advantage**: Each peak gets its own unique velocity trajectory, capturing local dynamics.

### Step 2: Co-accessibility Regularization

Use Cicero co-accessibility network to smooth velocity:

```python
for peak_i in all_peaks:
    # Find co-accessible peaks (score >= threshold)
    coaccess_neighbors = get_neighbors(peak_i, threshold=0.5)

    # Weighted average of neighbor velocities
    neighbor_velocity = weighted_average([
        velocity(neighbor_j) * coaccess_score(peak_i, neighbor_j)
        for neighbor_j in coaccess_neighbors
    ])

    # Combine temporal and co-accessibility
    regularized_velocity(peak_i) = (
        alpha * temporal_velocity(peak_i) +
        (1-alpha) * neighbor_velocity
    )
```

**Key advantage**: Incorporates regulatory context, reduces noise from isolated peaks.

### Step 3: 2D Projection

Local linear transformation for each peak:

```python
for peak_i in all_peaks:
    # Find k-nearest neighbors in 2D UMAP space
    neighbors = find_neighbors_2d(peak_i, k=30)

    # Compute local linear map: high-D velocity -> 2D displacement
    neighbor_velocities_hd = [velocity_hd(n) for n in neighbors]
    neighbor_displacements_2d = [umap(n) - umap(peak_i) for n in neighbors]

    # Solve: neighbor_velocities_hd @ W = neighbor_displacements_2d
    W = least_squares(neighbor_velocities_hd, neighbor_displacements_2d)

    # Apply to current peak
    velocity_2d(peak_i) = velocity_hd(peak_i) @ W
```

**Key advantage**: Preserves local structure, avoids global dimensionality reduction artifacts.

## üìà Expected Results

### Promoters
- Should maintain good performance from old method
- Clear directional flow from closed to open states
- Velocity aligned with developmental progression

### Enhancers/Intergenic
- **Expected improvement**: Local, cell-type specific dynamics captured
- Context-dependent velocity (not just global average)
- Regulatory relationships visible through co-accessibility

### Validation Metrics
- Velocity magnitude should be higher for:
  - Early developmental timepoints (rapid chromatin remodeling)
  - Neural/mesoderm cell types (high differentiation activity)
- Temporal vs regularized correlation: 0.7-0.9 (maintains signal while smoothing)

## üîç Troubleshooting

### Issue: High memory usage
**Solution**: Use `SUBSET_PEAKS` parameter to test on smaller dataset first
```python
SUBSET_PEAKS = 10000  # Start with 10K peaks
```

### Issue: Slow computation
**Solutions**:
1. Reduce `MAX_CONNECTIONS` (fewer co-accessible neighbors per peak)
2. Increase `MIN_COACCESS_SCORE` (fewer connections to process)
3. Use subset for parameter tuning, then run full dataset once

### Issue: Noisy velocity vectors
**Solutions**:
1. Increase `SMOOTHING_FACTOR` (0.5 ‚Üí 0.7 for smoother splines)
2. Increase `ALPHA` (give more weight to temporal over co-accessibility)
3. Increase `MIN_COACCESS_SCORE` (use only high-confidence connections)

### Issue: No UMAP coordinates in data
**Solution**: Algorithm will compute UMAP automatically using `scanpy.tl.umap()`
```python
# This happens automatically in project_to_2d() if needed
sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')
sc.tl.umap(adata)
```

## üìö Data Requirements

### Required Inputs
1. **Peaks-by-pseudobulk AnnData** (`peaks_by_pb_annotated_master.h5ad`)
   - Shape: (n_peaks, n_pseudobulks)
   - Pseudobulk naming: `{celltype_id}_{timepoint}` (e.g., `'0_5somites'`)
   - Obs columns: `peak_type`, `nearest_gene`, etc.
   - X matrix: Accessibility values (can be sparse)

2. **Co-accessibility DataFrame** (`cicero_connections.csv`)
   - Columns: `['Peak1', 'Peak2', 'coaccess']`
   - Peak1/Peak2: Peak names matching adata.obs_names
   - coaccess: Co-accessibility score (0-1)

### Optional
- UMAP coordinates in `adata.obsm['X_umap']` (will compute if not present)
- PCA representation in `adata.obsm['X_pca']` (will compute if not present)

## üéì Citation

When using this chromatin velocity implementation, please cite:

- **Zebrahub-Multiome paper**: [bioRxiv preprint](https://www.biorxiv.org/content/10.1101/2024.10.18.618987v1)
- **Cicero (co-accessibility)**: Pliner et al., Molecular Cell 2018
- **RNA velocity concept**: La Manno et al., Nature 2018; Bergen et al., Nature Biotechnology 2020

## üí° Future Directions

### Immediate Next Steps
1. Run on full dataset (all 640K peaks)
2. Validate against biological expectations (neural progenitors, mesoderm)
3. Compare to old implementation quantitatively
4. Optimize parameters using cross-validation

### Advanced Extensions
1. **Multi-resolution analysis**: Compute velocity at different temporal scales
2. **Cell-type specific networks**: Use different co-accessibility graphs per cell type
3. **Integration with RNA velocity**: Compare chromatin and transcription dynamics
4. **Trajectory alignment**: Project velocity along specific differentiation paths
5. **Driver peak identification**: Find peaks with highest velocity influence

### Integration with Existing Analysis
- **Peak UMAP clustering**: Overlay velocity on existing peak clusters
- **GRN analysis**: Connect to CellOracle regulatory networks
- **RNA-ATAC correlation**: Compare chromatin velocity with gene expression changes
- **In-silico perturbation**: Predict chromatin velocity changes after TF knockout

---

**Created**: October 2024
**Status**: Active development
**Contact**: Zebrahub-Multiome team

## üêõ Known Limitations

1. **Sparse timepoints**: Only 6 timepoints may limit derivative accuracy
   - Mitigation: Spline smoothing helps, but more timepoints would be ideal

2. **Computational cost**: Full dataset (640K peaks) requires significant memory
   - Mitigation: Use subset mode for testing, batch processing for full dataset

3. **Co-accessibility threshold sensitivity**: Results depend on `MIN_COACCESS_SCORE`
   - Mitigation: Parameter sweep recommended (try 0.3, 0.5, 0.7)

4. **Local projection approximation**: Assumes local linearity
   - Mitigation: Generally valid for local neighborhoods, but complex manifolds may cause issues

5. **Pseudobulk averaging**: May miss single-cell heterogeneity
   - Future: Extend to single-cell level if computational resources allow
